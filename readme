# ğŸ½ï¸ Student Project Report  
## Restaurant Review Sentiment Prediction (Khmerâ€“English Code-Mixed Text)

---

## ğŸ“Œ Overview
This project focuses on **sentiment classification of restaurant reviews** collected from Food Panda (Phnom Penh).  
The primary challenge is handling **Khmerâ€“English code-mixed text**, class imbalance, and noisy user-generated content.

**Objective:** Predict sentiment (**Negative / Neutral / Positive**) with high accuracy and balanced performance.

---

## ğŸ“Š Dataset Description

### Source
- **Dataset:** Cambodian Food Panda Restaurant Reviews (Phnom Penh)
- **Platform:** Kaggle
- **File:** `kh_phnom_penh_reviews.csv`
- **Total Samples:** ~14,647 reviews

### Key Columns
| Column | Description |
|------|------------|
| `text` | User review text |
| `overall` | Rating (1â€“5 stars) |
| `storeId` | Restaurant identifier |

### Language Characteristics
- English  
- Khmer  
- Khmerâ€“English Code-Mixed  

This linguistic diversity significantly complicates traditional NLP pipelines.

### Sentiment Label Mapping
| Rating | Sentiment |
|------|----------|
| 1â€“2 | Negative |
| 3 | Neutral |
| 4â€“5 | Positive |

---

## ğŸ” Exploratory Data Analysis (EDA)

### Sentiment Distribution
- **Positive:** ~50% (dominant)
- **Negative:** ~40%
- **Neutral:** ~10% (minority class)

### Language Analysis
A custom character-detection function identified:
- English-only reviews
- Khmer-only reviews
- Code-mixed reviews

### Visualizations
- `sentiment_distribution.png`
- `language_distribution.png`

These confirm **class imbalance** and **high linguistic noise**.

---

## ğŸ§¹ Preprocessing Pipeline

### 1ï¸âƒ£ Text Cleaning
Implemented via `clean_code_mixed_text()`:

- Unicode normalization (`NFKC`)
- Removal of:
  - URLs
  - Email addresses
  - Special characters
- Preservation of:
  - Khmer script (`\u1780â€“\u17FF`)
  - English characters (`aâ€“zAâ€“Z`)
- Lowercasing and whitespace normalization

### 2ï¸âƒ£ Label Encoding
Sentiment labels mapped to integers:
- Negative â†’ `0`
- Neutral â†’ `1`
- Positive â†’ `2`

### 3ï¸âƒ£ Feature Extraction (TF-IDF)
| Setting | Value |
|------|------|
| Analyzer | Character-level |
| N-gram Range | (2, 4) |
| Max Features | 5,000 |

**Rationale:**  
Character-level TF-IDF is robust against spelling variation, Khmer morphology, and code-mixed noise.

---

## ğŸ¤– Model Selection & Training

### Baseline Models
- Multinomial Naive Bayes
- Logistic Regression (`class_weight='balanced'`)
- Random Forest
- XGBoost (default)

### Advanced Models
- **LinearSVC**
  - Strong performance on high-dimensional sparse data
  - Calibrated with `CalibratedClassifierCV`
- **Tuned XGBoost**
  - `n_estimators = 200`
  - `max_depth = 7`
  - Custom sample weighting
- **Voting Classifier (Soft Voting)**
  - Logistic Regression
  - LinearSVC
  - XGBoost

---

## ğŸ¯ Fine-Tuning & Evaluation

### Class Imbalance Handling
- Automatic class balancing
- Manual Neutral class weight boost Ã— **1.5**

### Evaluation Metrics
- **Primary:** Accuracy
- **Secondary:** Weighted Precision, Recall, F1-Score
- **Critical Focus:** Neutral-class F1-Score

---

## ğŸ“ˆ Results & Discussion

### Model Performance Summary

| Model | Accuracy | Precision | Recall | F1-Score | Notes |
|------|---------|----------|--------|---------|------|
| **Voting Ensemble** | **80.89%** | 0.781 | 0.809 | **0.788** | **Best overall balance** |
| LinearSVC | 79.97% | 0.760 | 0.800 | 0.764 | Strong standalone model |
| Logistic Regression | 73.48% | 0.797 | 0.735 | 0.759 | High precision, weaker recall |
| XGBoost (Tuned) | 72.08% | **0.807** | 0.721 | 0.754 | Conservative predictions |

![Detailed Model Comparison](detailed_model_comparison.png)

### Key Insights
- **Voting Ensemble outperforms individual models**
- **LinearSVC excels with TF-IDF features**
- **Precisionâ€“Recall trade-offs** observed in boosted models

---

## âœ… Conclusion

- Achieved **80.89% accuracy**, surpassing the ~73% baseline
- Character-level TF-IDF + ensemble learning proved highly effective
- Neutral sentiment remains the most challenging class

### ğŸš€ Future Work
- Deep learning models (Bi-LSTM, Transformers)
- Data augmentation for Neutral reviews
- Sentence-level language detection

---

## ğŸ§  Final Verdict
The system demonstrates **robust performance** on noisy, multilingual, code-mixed text and provides a strong baseline for further research.
