# Student Project Report: Restaurant Review Sentiment Prediction

## 3. Methodology

### 3.1 Dataset Description
The dataset used in this project is the **Cambodian Food Panda Restaurants Reviews** (Phnom Penh subset), collected from Kaggle.
*   **Source File:** `kh_phnom_penh_reviews.csv`
*   **Total Samples:** ~14,647 reviews
*   **Key Columns:** `text` (the reviewer's comment), `overall` (rating 1-5), and metadata like `storeId`.
*   **Language Composition:** The dataset is linguistically diverse, containing **English**, **Khmer** (Cambodian), and **Code-Mixed** (both languages in one review) text. This poses a unique challenge for natural language processing.
*   **Labeling:** Ratings were converted into sentiment classes:
    *   **Negative:** 1-2 stars
    *   **Neutral:** 3 stars
    *   **Positive:** 4-5 stars

### 3.2 Exploratory Data Analysis (EDA)
We explored the data to understand class balance and language distribution.
*   **Sentiment Distribution:** The dataset is imbalanced.
    *   **Positive:** Dominant class (~50%)
    *   **Negative:** Significant portion (~40%)
    *   **Neutral:** Minority class (~10%), making it the hardest to predict.
*   **Language Analysis:** A custom function detected Khmer and English characters. A significant portion of reviews are English-only or Khmer-English mixed.
*   **Visualization:** Bar charts and pie charts were generated to visualize these distributions (see `language_distribution.png` and `sentiment_distribution.png`).

### 3.3 Preprocessing
To prepare the raw text for machine learning models, the following preprocessing steps were applied:

1.  **Text Cleaning (`clean_code_mixed_text` function):**
    *   **Unicode Normalization:** Normalized text to `NFKC` form.
    *   **Noise Removal:** Removed URLs, email addresses, and non-alphanumeric characters (preserving Khmer script `\u1780-\u17FF` and English `a-zA-Z`).
    *   **Standardization:** Converted all text to lowercase and stripped extra whitespace.
2.  **Label Encoding:** Converted string labels ('negative', 'neutral', 'positive') into integers (0, 1, 2) using `LabelEncoder`.
3.  **Feature Extraction (TF-IDF):**
    *   **Vectorization:** Used `TfidfVectorizer`.
    *   **Analyzer:** `char` (Character-level n-grams) was chosen over word-level to better handle the agglutinative nature of Khmer and the spelling variations in code-mixed text.
    *   **N-gram Range:** (2, 4) characters.
    *   **Max Features:** Limited to 5,000 top features to control dimensionality.

### 3.4 Model Selection and Training
We evaluated a diverse set of machine learning models to identify the best performer.

1.  **Baseline Models:**
    *   **Naive Bayes (MultinomialNB):** A standard baseline for text classification.
    *   **Logistic Regression:** Robust linear model, trained with `class_weight='balanced'` to handle imbalance.
    *   **Random Forest:** Ensemble bagging method.
    *   **XGBoost (Original):** Gradient boosting machine.

2.  **Advanced Models (Improvements):**
    *   **LinearSVC (Support Vector Classifier):** Known for efficacy in high-dimensional text data. Implemented with `CalibratedClassifierCV` to provide probability outputs.
    *   **Tuned XGBoost:** Hyperparameters optimized (`n_estimators=200`, `max_depth=7`) and trained with specific sample weights.
    *   **Voting Classifier:** An ensemble combining Logistic Regression, LinearSVC, and XGBoost using soft voting to leverage the strengths of each.

### 3.5 Model Fine-tuning and Evaluation
*   **Class Weighting:** To address the low performance on the Neutral class, we applied `class_weight='balanced'` and further experimented with manually boosting the Neutral class weight by a factor of 1.5.
*   **Evaluation Metric:** The primary metric for success was **Accuracy**. However, we closely monitored the **Neutral F1 Score** since classifying neutral reviews is the primary challenge in this dataset.

## 4. Result and Discussion

The following table summarizes the comprehensive performance of the evaluated models (weighted averages for Precision, Recall, F1):

| Model | Accuracy | Precision | Recall | F1-Score | Observations |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Voting Ensemble** | **80.89%** | 0.781 | 0.809 | **0.788** | **Best Overall.** Effectively balances the strengths of LinearSVC and others. |
| **LinearSVC** | 79.97% | 0.760 | 0.800 | 0.764 | High accuracy, but slightly lower F1-Score, indicating some trade-offs in class balance. |
| **Logistic Regression** | 73.48% | 0.797 | 0.735 | 0.759 | Good precision but suffers in recall compared to the top performers. |
| **XGBoost (Tuned)** | 72.08% | **0.807** | 0.721 | 0.754 | Highest precision but lowest accuracy/recall; tends to be conservative. |

![Detailed Model Comparison](detailed_model_comparison.png)

**Discussion:**
*   **Voting Classifier is the Winner:** It achieved the highest **Accuracy (80.89%)** and **F1-Score (0.788)**. By combining the probability outputs of Logistic Regression, LinearSVC, and XGBoost (Soft Voting), it smoothed out individual model errors, leading to more robust predictions.
*   **LinearSVC Strength:** LinearSVC remains a very strong individual contender with ~80% accuracy, proving that the high-dimensional TF-IDF features are well-separated linearly.
*   **Precision vs. Recall Trade-off:** Tuned XGBoost achieved the highest Precision (80.7%), meaning when it predicts a sentiment, it's usually right, but it misses many cases (lower Recall 72%). The Voting model strikes the best balance.

## 5. Conclusion
In this project, we successfully developed a sentiment analysis model for Khmer-English code-mixed restaurant reviews.
*   **Key Achievement:** We surpassed the initial baseline of ~73% accuracy, achieving **80.89%** with a **Voting Ensemble** of LinearSVC, Logistic Regression, and XGBoost.
*   **Insight:** Character-level TF-IDF features combined with ensemble learning proved highly effective for handling the noise and complexity of code-mixed text.
*   **Future Work:** While accuracy is high, further improvements could focus on the minority "Neutral" class using advanced deep learning techniques (like LSTM or Transformers) or targeted data augmentation.
